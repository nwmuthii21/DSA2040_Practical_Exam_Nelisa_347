{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b58e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from faker import Faker\n",
    "    HAS_FAKER = True\n",
    "except ImportError:\n",
    "    HAS_FAKER = False\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    ")\n",
    "\n",
    "CURRENT_DATE = datetime(2025, 8, 12)\n",
    "\n",
    "\n",
    "# 0. Synthetic data generation\n",
    "# -------------------------------------------------------------------\n",
    "def generate_synthetic_data(n_rows: int = 1000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate synthetic retail invoice-like data.\n",
    "    Columns: InvoiceNo, InvoiceDate, CustomerID, Country, ProductID,\n",
    "             Quantity, UnitPrice.\n",
    "    \"\"\"\n",
    "    logging.info(\"Generating synthetic data with %d rows...\", n_rows)\n",
    "\n",
    "    if HAS_FAKER:\n",
    "        fake = Faker()\n",
    "        Faker.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Basic ranges\n",
    "    n_customers = 100\n",
    "    n_products = 200\n",
    "    countries = [\n",
    "        \"Kenya\", \"Uganda\", \"Tanzania\", \"South Africa\", \"Nigeria\",\n",
    "        \"USA\", \"UK\", \"Germany\", \"India\", \"Brazil\"\n",
    "    ]\n",
    "\n",
    "    data = {}\n",
    "\n",
    " # InvoiceNo: simple integer or string\n",
    "    data[\"InvoiceNo\"] = [f\"INV-{100000 + i}\" for i in range(n_rows)]\n",
    "\n",
    "    # InvoiceDate: random dates over last 2 years from CURRENT_DATE\n",
    "    days_back = 365 * 2\n",
    "    random_days = np.random.randint(0, days_back, size=n_rows)\n",
    "    data[\"InvoiceDate\"] = [\n",
    "        (CURRENT_DATE - timedelta(days=int(d))).strftime(\"%Y-%m-%d\")\n",
    "        for d in random_days\n",
    "    ]\n",
    "\n",
    "    # CustomerID: pick from 1..100\n",
    "    data[\"CustomerID\"] = np.random.randint(1, n_customers + 1, size=n_rows)\n",
    "\n",
    "    # Country: random from list\n",
    "    data[\"Country\"] = np.random.choice(countries, size=n_rows)\n",
    "\n",
    " # ProductID: pick from 1..200\n",
    "    data[\"ProductID\"] = np.random.randint(1, n_products + 1, size=n_rows)\n",
    "\n",
    "    # Quantity: 1..50 (some possible bad values for outlier test)\n",
    "    data[\"Quantity\"] = np.random.randint(1, 51, size=n_rows)\n",
    "\n",
    "    # UnitPrice: 1..100 as float with 2 decimals\n",
    "    data[\"UnitPrice\"] = np.round(\n",
    "        np.random.uniform(1, 100, size=n_rows), 2\n",
    "    )\n",
    "     # Introduce a few bad rows (Quantity < 0 or UnitPrice <= 0) for testing\n",
    "    bad_indices = np.random.choice(n_rows, size=10, replace=False)\n",
    "    for idx in bad_indices[:5]:\n",
    "        data[\"Quantity\"][idx] = -abs(data[\"Quantity\"][idx])\n",
    "    for idx in bad_indices[5:]:\n",
    "        data[\"UnitPrice\"][idx] = 0.0\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    logging.info(\"Synthetic data generated: %d rows.\", len(df))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00fdf216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract\n",
    "\n",
    "def extract_data(csv_path: str | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract data into a pandas DataFrame.\n",
    "    If csv_path is provided and exists, read from CSV,\n",
    "    otherwise generate synthetic data.\n",
    "    \"\"\"\n",
    "    if csv_path and os.path.exists(csv_path):\n",
    "        logging.info(\"Reading data from CSV: %s\", csv_path)\n",
    "        df = pd.read_csv(csv_path)\n",
    "    else:\n",
    "        logging.info(\"CSV not provided or not found. Using synthetic data.\")\n",
    "        df = generate_synthetic_data(1000)\n",
    "\n",
    "    logging.info(\"Extracted rows: %d\", len(df))\n",
    "\n",
    "    # Handle missing values (simple example: drop rows with all NaNs)\n",
    "    df = df.dropna(how=\"all\")\n",
    "    logging.info(\"After dropping fully empty rows: %d\", len(df))\n",
    "\n",
    "    # Convert InvoiceDate to datetime\n",
    "    if \"InvoiceDate\" in df.columns:\n",
    "         df[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"], errors=\"coerce\")\n",
    "        # Drop rows with invalid dates\n",
    "    df = df.dropna(subset=[\"InvoiceDate\"])\n",
    "\n",
    "    logging.info(\"After date conversion: %d rows\", len(df))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ae5f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Transform\n",
    "\n",
    "def transform_data(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Transform the extracted DataFrame:\n",
    "      - Handle outliers\n",
    "      - Compute TotalSales\n",
    "      - Filter last-year sales (from CURRENT_DATE)\n",
    "      - Create CustomerDim-like summary\n",
    "      - Create TimeDim-like DataFrame\n",
    "    Returns:\n",
    "      fact_df, customer_dim_df, time_dim_df\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting transformation on %d rows\", len(df))\n",
    "\n",
    "    # Handle outliers: remove Quantity < 0 or UnitPrice <= 0\n",
    "    before_outlier = len(df)\n",
    "    df = df[(df[\"Quantity\"] >= 0) & (df[\"UnitPrice\"] > 0)]\n",
    "    after_outlier = len(df)\n",
    "    logging.info(\n",
    "        \"Removed %d rows as outliers (Quantity < 0 or UnitPrice <= 0).\",\n",
    "        before_outlier - after_outlier\n",
    "    )\n",
    " # TotalSales = Quantity * UnitPrice\n",
    "    df[\"TotalSales\"] = df[\"Quantity\"] * df[\"UnitPrice\"]\n",
    "\n",
    "    # Filter for sales in the last year (from CURRENT_DATE)\n",
    "    one_year_ago = CURRENT_DATE - timedelta(days=365)\n",
    "    mask_last_year = df[\"InvoiceDate\"] >= one_year_ago\n",
    "    df_last_year = df[mask_last_year].copy()\n",
    "    logging.info(\n",
    "        \"Filtered to last year (%s to %s): %d rows\",\n",
    "        one_year_ago.date(),\n",
    "        CURRENT_DATE.date(),\n",
    "        len(df_last_year)\n",
    "    )\n",
    " # Customer summary: group by CustomerID\n",
    "    customer_dim_df = (\n",
    "        df_last_year\n",
    "        .groupby([\"CustomerID\", \"Country\"], as_index=False)\n",
    "        .agg(\n",
    "            TotalPurchases=(\"TotalSales\", \"sum\"),\n",
    "            TotalQuantity=(\"Quantity\", \"sum\"),\n",
    "            OrderCount=(\"InvoiceNo\", \"nunique\")\n",
    "        )\n",
    "    )\n",
    "    logging.info(\n",
    "        \"CustomerDim rows created: %d unique customers\",\n",
    "        len(customer_dim_df)\n",
    "    )\n",
    "\n",
    " # Time dimension from unique InvoiceDate values\n",
    "    time_dim_df = (\n",
    "        df_last_year[[\"InvoiceDate\"]]\n",
    "        .drop_duplicates()\n",
    "        .copy()\n",
    "    )\n",
    "    time_dim_df[\"TimeID\"] = range(1, len(time_dim_df) + 1)\n",
    "    time_dim_df[\"Date\"] = time_dim_df[\"InvoiceDate\"].dt.date\n",
    "    time_dim_df[\"Day\"] = time_dim_df[\"InvoiceDate\"].dt.day\n",
    "    time_dim_df[\"Month\"] = time_dim_df[\"InvoiceDate\"].dt.month\n",
    "    time_dim_df[\"Year\"] = time_dim_df[\"InvoiceDate\"].dt.year\n",
    "    time_dim_df[\"Quarter\"] = time_dim_df[\"InvoiceDate\"].dt.quarter\n",
    "    logging.info(\"TimeDim rows created: %d unique dates\", len(time_dim_df))\n",
    "\n",
    "  # Create a mapping from date to TimeID for fact table\n",
    "    date_to_timeid = dict(\n",
    "        zip(time_dim_df[\"Date\"], time_dim_df[\"TimeID\"])\n",
    "    )\n",
    "\n",
    "    # Fact table: map InvoiceDate -> TimeID\n",
    "    fact_df = df_last_year.copy()\n",
    "    fact_df[\"Date\"] = fact_df[\"InvoiceDate\"].dt.date\n",
    "    fact_df[\"TimeID\"] = fact_df[\"Date\"].map(date_to_timeid)\n",
    "# Rename columns to be more warehouse-like\n",
    "    fact_df = fact_df.rename(\n",
    "        columns={\n",
    "            \"CustomerID\": \"customer_id\",\n",
    "            \"ProductID\": \"product_id\",\n",
    "            \"Quantity\": \"quantity\",\n",
    "            \"UnitPrice\": \"unit_price\",\n",
    "            \"TotalSales\": \"total_sales\",\n",
    "            \"InvoiceNo\": \"invoice_no\",\n",
    "            \"Country\": \"country\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    logging.info(\"Fact table rows: %d\", len(fact_df))\n",
    " # Prepare CustomerDim structure\n",
    "    customer_dim_df = customer_dim_df.rename(\n",
    "        columns={\n",
    "            \"CustomerID\": \"customer_id\",\n",
    "            \"Country\": \"country\",\n",
    "            \"TotalPurchases\": \"total_purchases\",\n",
    "            \"TotalQuantity\": \"total_quantity\",\n",
    "            \"OrderCount\": \"order_count\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Prepare TimeDim structure\n",
    "    time_dim_df = time_dim_df[\n",
    "        [\"TimeID\", \"Date\", \"Day\", \"Month\", \"Quarter\", \"Year\"]\n",
    "    ].rename(\n",
    "        columns={\n",
    "            \"TimeID\": \"time_id\",\n",
    "            \"Date\": \"date\",\n",
    "            \"Day\": \"day\",\n",
    "            \"Month\": \"month\",\n",
    "            \"Quarter\": \"quarter\",\n",
    "            \"Year\": \"year\"\n",
    "        }\n",
    "    )\n",
    "    return fact_df, customer_dim_df, time_dim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14be1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load\n",
    "\n",
    "def load_to_sqlite(\n",
    "    fact_df: pd.DataFrame,\n",
    "    customer_dim_df: pd.DataFrame,\n",
    "    time_dim_df: pd.DataFrame,\n",
    "    db_path: str = \"retail_dw.db\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Load DataFrames into SQLite:\n",
    "      - SalesFact\n",
    "      - CustomerDim\n",
    "      - TimeDim\n",
    "    Overwrites tables if they exist.\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading data into SQLite DB: %s\", db_path)\n",
    "\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    try:\n",
    "        # Write dimension tables first\n",
    "        customer_dim_df.to_sql(\n",
    "            \"CustomerDim\",\n",
    "            conn,\n",
    "            if_exists=\"replace\",\n",
    "            index=False\n",
    "        )\n",
    "        time_dim_df.to_sql(\n",
    "            \"TimeDim\",\n",
    "            conn,\n",
    "            if_exists=\"replace\",\n",
    "            index=False\n",
    "        )\n",
    "         # Then fact table\n",
    "        fact_df.to_sql(\n",
    "            \"SalesFact\",\n",
    "            conn,\n",
    "            if_exists=\"replace\",\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "        logging.info(\"Loaded CustomerDim rows: %d\", len(customer_dim_df))\n",
    "        logging.info(\"Loaded TimeDim rows: %d\", len(time_dim_df))\n",
    "        logging.info(\"Loaded SalesFact rows: %d\", len(fact_df))\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error while loading to SQLite: %s\", e)\n",
    "        raise\n",
    "    finally:\n",
    "        conn.close()\n",
    "        logging.info(\"SQLite connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ab38a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Full ETL function\n",
    "\n",
    "def run_etl(csv_path: str | None = None, db_path: str = \"retail_dw.db\"):\n",
    "    \"\"\"\n",
    "    Run the full ETL:\n",
    "      1. Extract from CSV or generate synthetic\n",
    "      2. Transform (clean, outliers, TotalSales, dimensions)\n",
    "      3. Load into SQLite\n",
    "    Logs row counts at each stage.\n",
    "    \"\"\"\n",
    "    logging.info(\"=== ETL STARTED ===\")\n",
    "\n",
    "    # Extract\n",
    "    df_raw = extract_data(csv_path=csv_path)\n",
    "    logging.info(\"Raw row count after extract: %d\", len(df_raw))\n",
    "\n",
    "    # Transform\n",
    "    fact_df, customer_dim_df, time_dim_df = transform_data(df_raw)\n",
    "    logging.info(\n",
    "        \"After transform - Fact: %d, CustomerDim: %d, TimeDim: %d\",\n",
    "        len(fact_df), len(customer_dim_df), len(time_dim_df)\n",
    "    )\n",
    " # Load\n",
    "    load_to_sqlite(\n",
    "        fact_df=fact_df,\n",
    "        customer_dim_df=customer_dim_df,\n",
    "        time_dim_df=time_dim_df,\n",
    "        db_path=db_path\n",
    "    )\n",
    "\n",
    "    logging.info(\"=== ETL COMPLETED SUCCESSFULLY ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "476de319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 15:15:52,893 [INFO] === ETL STARTED ===\n",
      "2025-12-12 15:15:52,900 [INFO] CSV not provided or not found. Using synthetic data.\n",
      "2025-12-12 15:15:52,904 [INFO] Generating synthetic data with 1000 rows...\n",
      "2025-12-12 15:15:52,967 [INFO] Synthetic data generated: 1000 rows.\n",
      "2025-12-12 15:15:52,973 [INFO] Extracted rows: 1000\n",
      "2025-12-12 15:15:52,979 [INFO] After dropping fully empty rows: 1000\n",
      "2025-12-12 15:15:53,006 [INFO] After date conversion: 1000 rows\n",
      "2025-12-12 15:15:53,006 [INFO] Raw row count after extract: 1000\n",
      "2025-12-12 15:15:53,012 [INFO] Starting transformation on 1000 rows\n",
      "2025-12-12 15:15:53,020 [INFO] Removed 10 rows as outliers (Quantity < 0 or UnitPrice <= 0).\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12108\\585735115.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"TotalSales\"] = df[\"Quantity\"] * df[\"UnitPrice\"]\n",
      "2025-12-12 15:15:53,029 [INFO] Filtered to last year (2024-08-12 to 2025-08-12): 495 rows\n",
      "2025-12-12 15:15:53,036 [INFO] CustomerDim rows created: 388 unique customers\n",
      "2025-12-12 15:15:53,052 [INFO] TimeDim rows created: 272 unique dates\n",
      "2025-12-12 15:15:53,056 [INFO] Fact table rows: 495\n",
      "2025-12-12 15:15:53,060 [INFO] After transform - Fact: 495, CustomerDim: 388, TimeDim: 272\n",
      "2025-12-12 15:15:53,062 [INFO] Loading data into SQLite DB: retail_dw.db\n",
      "2025-12-12 15:15:53,154 [INFO] Loaded CustomerDim rows: 388\n",
      "2025-12-12 15:15:53,154 [INFO] Loaded TimeDim rows: 272\n",
      "2025-12-12 15:15:53,154 [INFO] Loaded SalesFact rows: 495\n",
      "2025-12-12 15:15:53,154 [INFO] SQLite connection closed.\n",
      "2025-12-12 15:15:53,154 [INFO] === ETL COMPLETED SUCCESSFULLY ===\n"
     ]
    }
   ],
   "source": [
    "# Script entry point\n",
    "# -------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # If you want to use a real CSV, provide path here:\n",
    "    # run_etl(csv_path=\"your_file.csv\")\n",
    "    run_etl()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e38717f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Product_1</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Product_2</td>\n",
       "      <td>Beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Product_3</td>\n",
       "      <td>Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Product_4</td>\n",
       "      <td>Beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Product_5</td>\n",
       "      <td>Beauty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id product_name category\n",
       "0           1    Product_1   Sports\n",
       "1           2    Product_2   Beauty\n",
       "2           3    Product_3     Home\n",
       "3           4    Product_4   Beauty\n",
       "4           5    Product_5   Beauty"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Connect to SQLite\n",
    "conn = sqlite3.connect(\"retail_dw.db\")\n",
    "\n",
    "# Generate synthetic product data\n",
    "np.random.seed(42)\n",
    "\n",
    "num_products = 200\n",
    "categories = [\"Electronics\", \"Clothing\", \"Home\", \"Sports\", \"Beauty\"]\n",
    "\n",
    "product_df = pd.DataFrame({\n",
    "    \"product_id\": range(1, num_products + 1),\n",
    "    \"product_name\": [f\"Product_{i}\" for i in range(1, num_products + 1)],\n",
    "    \"category\": np.random.choice(categories, size=num_products)\n",
    "})\n",
    "\n",
    "# Load into SQLite\n",
    "product_df.to_sql(\"dim_product\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "product_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
